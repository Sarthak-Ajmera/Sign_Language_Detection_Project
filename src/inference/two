import cv2  
import numpy as np  
from tensorflow.keras.models import load_model  
import mediapipe as mp  
from gtts import gTTS  
import pygame  
from collections import deque  

# Load trained model
model = load_model("C:\\sign_language_project\\models\\saved_models\\best_model.h5")

# Class labels (A-Z) 
class_labels = [chr(i) for i in range(ord('A'), ord('Z') + 1)]

# Initialize MediaPipe Hands
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)

# Initialize webcam
cap = cv2.VideoCapture(0)
if not cap.isOpened():
    print("âŒ Error: Could not open webcam.")
    exit()

# Variables to store sentence
current_text = ""
prediction_buffer = deque(maxlen=5)  # Store last 5 predictions for smoothing

def preprocess_image(frame):
    """Preprocess image for model."""
    img = cv2.resize(frame, (64, 64))
    img = img / 255.0  # Normalize
    img = np.expand_dims(img, axis=0)
    return img

def predict_image(frame):
    """Predict sign only if a hand is detected and use smoothing."""
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    rgb_frame = cv2.flip(rgb_frame, 1)  # Flip image for correct orientation
    results = hands.process(rgb_frame)

    if results.multi_hand_landmarks:
        for hand_landmarks in results.multi_hand_landmarks:
            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)
        
        img = preprocess_image(frame)
        prediction = model.predict(img)
        predicted_class = np.argmax(prediction, axis=1)[0]
        predicted_label = class_labels[predicted_class]
        confidence = np.max(prediction) * 100

        # Add prediction to buffer for stability
        prediction_buffer.append(predicted_label)

        # Return the most frequent prediction in buffer
        most_common_prediction = max(set(prediction_buffer), key=prediction_buffer.count)
        return most_common_prediction, confidence
    else:
        return None, 0  # No hand detected

def text_to_speech(text):
    """Convert text to speech (speaks full words)."""
    if not text.strip():
        return  # Don't speak if text is empty
    
    print(f"ğŸ—£ Speaking: '{text}'")
    tts = gTTS(text=text, lang='en')
    tts.save("output.mp3")
    
    pygame.mixer.init()
    pygame.mixer.music.load("output.mp3")
    pygame.mixer.music.play()
    
    # Wait for playback to finish
    while pygame.mixer.music.get_busy():
        pygame.time.Clock().tick(10)

# Real-time detection loop
while True:
    ret, frame = cap.read()
    if not ret:
        print("âŒ Error: Failed to capture frame.")
        break

    cv2.putText(frame, "Press 'p' to add letter", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
    cv2.putText(frame, "Press SPACE to add space", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
    cv2.putText(frame, "Press 's' to speak full sentence", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
    cv2.putText(frame, "Press 'd' to delete | 'c' to clear", (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
    cv2.putText(frame, "Press 'q' to quit", (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
    cv2.putText(frame, f"Text: {current_text}", (10, 190), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)

    key = cv2.waitKey(1) & 0xFF
    if key == ord('p'):
        predicted_label, confidence = predict_image(frame)
        if predicted_label:
            if confidence > 70:  # Confidence threshold to reduce errors
                current_text += predicted_label
                print(f"âœ… Added: {predicted_label} | Current text: {current_text}")
        else:
            print("âš  No hand detected. Try again.")
    
    elif key == 32:  # Spacebar key code
        current_text += " "
        print(f"â£ Added space | Current text: {current_text}")

    elif key == ord('d') and current_text:
        current_text = current_text[:-1]
        print(f"ğŸ”™ Deleted last character | Current text: {current_text}")

    elif key == ord('c'):
        current_text = ""
        print("ğŸ§¹ Cleared all text.")

    elif key == ord('s') and current_text:
        # Speak the full sentence (all words)
        text_to_speech(current_text)

    elif key == ord('q'):
        break

    cv2.imshow("Sign Language Detection", frame)

# Cleanup
cap.release()
cv2.destroyAllWindows()